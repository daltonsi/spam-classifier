{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f33133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apache Spam Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799318e",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07d9e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "URL_ROOT = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
    "HAM_FILE = '20021010_easy_ham.tar.bz2'\n",
    "SPAM_FILE = '20021010_spam.tar.bz2'\n",
    "\n",
    "\n",
    "MAIN_PATH = os.getcwd()\n",
    "SOURCE_PATH = os.path.join(MAIN_PATH, 'data/source/')\n",
    "\n",
    "def reset_directory(directory):\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    os.makedirs(directory) \n",
    "\n",
    "def download_spam():\n",
    "    \"\"\" Download Spam/Ham sources into local directory \"\"\"\n",
    "    reset_directory(SOURCE_PATH)\n",
    "    \n",
    "    for filename in [HAM_FILE, SPAM_FILE]:\n",
    "        r = requests.get(URL_ROOT + filename)\n",
    "        filepath = os.path.join(SOURCE_PATH, filename)\n",
    "        with open(filepath, 'wb') as destination:\n",
    "            destination.write(r.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f5bccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_spam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4cbd5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "EMAILS_PATH = os.path.join(SOURCE_PATH, 'emails/')\n",
    "\n",
    "def extract_spam():\n",
    "    \"\"\" Extracts spam email files from compressed directory \"\"\"\n",
    "    reset_directory(EMAILS_PATH)\n",
    "    \n",
    "    for file in os.listdir(SOURCE_PATH):\n",
    "        if file.endswith(\".bz2\"): \n",
    "            filepath = os.path.join(SOURCE_PATH, file)\n",
    "            tar = tarfile.open(filepath, mode='r:bz2')\n",
    "            tar.extractall(EMAILS_PATH)\n",
    "        else:\n",
    "             continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a85aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0620c3",
   "metadata": {},
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15a52c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "TRAINING_PATH = os.path.join(MAIN_PATH, 'data/training/source/')\n",
    "TEST_PATH = os.path.join(MAIN_PATH, 'data/test/source/')\n",
    "\n",
    "EMAIL_SPAM_PATH = os.path.join(EMAILS_PATH, 'spam/')\n",
    "EMAIL_HAM_PATH = os.path.join(EMAILS_PATH, 'easy_ham/')\n",
    "\n",
    "NUM_SPAM = len(os.listdir(EMAIL_SPAM_PATH))\n",
    "NUM_HAM = len(os.listdir(EMAIL_HAM_PATH))\n",
    "\n",
    "\n",
    "\n",
    "def split_spam_train_test(seed=50, percent_test=20):\n",
    "    \"\"\" Splits spam email files into test/train directories \"\"\"\n",
    "    reset_directory(TRAINING_PATH)\n",
    "    reset_directory(TEST_PATH)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    spam_test_indcs = random.sample([i for i in range(NUM_SPAM)], k=int(NUM_SPAM*percent_test/100))\n",
    "    ham_test_indcs = random.sample([i for i in range(NUM_HAM)], k=int(NUM_HAM*percent_test/100))\n",
    "    \n",
    "    for file in os.listdir(EMAIL_SPAM_PATH):\n",
    "        _id = int(file.split('.')[0])\n",
    "        source_path = os.path.join(EMAIL_SPAM_PATH, file)\n",
    "\n",
    "        if _id in spam_test_indcs:\n",
    "            destination = os.path.join(TEST_PATH, file + '.spam')\n",
    "            os.popen(f\"cp {source_path} {destination}\") \n",
    "\n",
    "        else:\n",
    "            destination = os.path.join(TRAINING_PATH, file + '.spam')\n",
    "            os.popen(f\"cp {source_path} {destination}\") \n",
    "\n",
    "        \n",
    "    for file in os.listdir(EMAIL_HAM_PATH):\n",
    "        _id = int(file.split('.')[0])\n",
    "        source_path = os.path.join(EMAIL_HAM_PATH, file)\n",
    "\n",
    "        if _id in ham_test_indcs:\n",
    "            destination = os.path.join(TEST_PATH, file + '.ham')\n",
    "            os.popen(f\"cp {source_path} {destination}\") \n",
    "\n",
    "        else:\n",
    "            destination = os.path.join(TRAINING_PATH, file + '.ham')\n",
    "            os.popen(f\"cp {source_path} {destination}\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7bcc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_spam_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "887a0b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/daltonsi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/daltonsi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import email\n",
    "import urlextract\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# REF: https://wkirgsn.github.io/2018/02/15/pandas-pipelines/    \n",
    "class SpamTokenCounter(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Transforms each email of a spam or ham directory into a Counter of counted tokens \"\"\"\n",
    "    def __init__(self, remove_urls=True, remove_stop=True, agg_ints=True, agg_leading_zeros=True, stemmer=True):\n",
    "        self.remove_urls = remove_urls\n",
    "        self.remove_stop = remove_stop\n",
    "        self.agg_ints = agg_ints\n",
    "        self.agg_leading_zeros = agg_leading_zeros\n",
    "        self.stemmer = stemmer\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def process_content(self, mail_objc):\n",
    "        part_type = mail_objc.get_content_type()\n",
    "        if part_type == 'text/plain':\n",
    "            return mail_objc.get_payload()\n",
    "        elif part_type == 'text/html':\n",
    "            html = mail_objc.get_payload()\n",
    "            soup = BeautifulSoup(html, features=\"html.parser\")\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract() \n",
    "            if soup.body == None:\n",
    "                return ''\n",
    "            return soup.body.get_text()\n",
    "        else:\n",
    "            return ''\n",
    "    \n",
    "    def retrieve_text_content(self, x_msg):\n",
    "         if x_msg.is_multipart():\n",
    "            prcssd_msg = ''\n",
    "            for msg in x_msg.walk():\n",
    "                payload = self.process_content(msg)\n",
    "                if payload:\n",
    "                    prcssd_msg += payload\n",
    "            \n",
    "            return prcssd_msg\n",
    "         else:\n",
    "            return self.process_content(x_msg)       \n",
    "  \n",
    "    def tokenize_and_count(self, text: str):\n",
    "        \n",
    "        if self.remove_urls:\n",
    "            url_extractor = urlextract.URLExtract()\n",
    "            urls = list(set(url_extractor.find_urls(text)))\n",
    "            for url in urls:\n",
    "                text = text.replace(url, \" URL \")\n",
    "        \n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", ' ', text) #alphanumeric lowercase\n",
    "        tokens = word_tokenize(text)\n",
    "        if self.agg_ints:\n",
    "            tokens = [re.sub(r\"^[0-9]+$\", 'INTEGER', token) for token in tokens]\n",
    "        if self.agg_leading_zeros:\n",
    "            tokens = [re.sub(r\"^[0]+.*\", 'LEADING_ZERO', token) for token in tokens]\n",
    "        if self.remove_stop:\n",
    "            tokens = [token for token in tokens if token not in stopwords.words(\"english\")]\n",
    "        if self.stemmer:\n",
    "            tokens = [PorterStemmer().stem(w) for w in tokens]\n",
    "\n",
    "        return Counter(tokens)  \n",
    "\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_transformed = []\n",
    "        for file in os.listdir(X):\n",
    "            if not file.startswith('.'):\n",
    "                isSpam = file.split('.')[-1] == 'spam'\n",
    "                msg_objc = email.message_from_string(open(os.path.join(X,file), encoding='latin1').read())\n",
    "                msg_content = self.retrieve_text_content(msg_objc)\n",
    "                token_counter = self.tokenize_and_count(msg_content)                \n",
    "                X_transformed.append([isSpam,token_counter])\n",
    "        return X_transformed\n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bce863c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_token_counter = SpamTokenCounter()\n",
    "training_spam_token_counters = spam_token_counter.fit_transform(TRAINING_PATH)\n",
    "test_spam_token_counters = spam_token_counter.fit_transform(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf3f7ecf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[False,\n",
       "  Counter({'how': 1,\n",
       "           'easi': 2,\n",
       "           'would': 1,\n",
       "           'code': 4,\n",
       "           'menu': 2,\n",
       "           'item': 1,\n",
       "           'mark': 1,\n",
       "           'read': 3,\n",
       "           'messag': 5,\n",
       "           'current': 2,\n",
       "           'i': 9,\n",
       "           'often': 1,\n",
       "           'problem': 1,\n",
       "           'heavi': 1,\n",
       "           'traffic': 1,\n",
       "           'mail': 2,\n",
       "           'list': 3,\n",
       "           'exmh': 11,\n",
       "           'want': 2,\n",
       "           'subset': 1,\n",
       "           'lose': 1,\n",
       "           'track': 1,\n",
       "           'particular': 2,\n",
       "           'folder': 2,\n",
       "           'thi': 1,\n",
       "           'could': 1,\n",
       "           'also': 1,\n",
       "           'gener': 1,\n",
       "           'new': 1,\n",
       "           'sequenc': 2,\n",
       "           'way': 1,\n",
       "           'remov': 1,\n",
       "           'first': 1,\n",
       "           'excus': 1,\n",
       "           'still': 2,\n",
       "           'use': 1,\n",
       "           'integ': 7,\n",
       "           'may': 1,\n",
       "           'play': 1,\n",
       "           'well': 1,\n",
       "           'recent': 1,\n",
       "           'chang': 1,\n",
       "           'look': 2,\n",
       "           'and': 1,\n",
       "           'top': 1,\n",
       "           'head': 2,\n",
       "           'see': 1,\n",
       "           'work': 2,\n",
       "           'somewhat': 1,\n",
       "           'untest': 1,\n",
       "           'ad': 1,\n",
       "           'entri': 1,\n",
       "           'trivial': 1,\n",
       "           'add': 1,\n",
       "           'default': 1,\n",
       "           'fop': 3,\n",
       "           'uentrylist': 1,\n",
       "           'ketchup': 3,\n",
       "           'l': 1,\n",
       "           'catch': 1,\n",
       "           'b4': 1,\n",
       "           'cur': 3,\n",
       "           'c': 1,\n",
       "           'my': 3,\n",
       "           'mark2curseen': 3,\n",
       "           'then': 1,\n",
       "           'need': 2,\n",
       "           'provid': 1,\n",
       "           'sourc': 2,\n",
       "           'arrang': 1,\n",
       "           'includ': 1,\n",
       "           'put': 1,\n",
       "           'tk': 1,\n",
       "           'directori': 1,\n",
       "           'user': 1,\n",
       "           'tcl': 1,\n",
       "           'pick': 5,\n",
       "           'patch': 1,\n",
       "           'enabl': 1,\n",
       "           'hook': 1,\n",
       "           'pref': 1,\n",
       "           'hack': 1,\n",
       "           'support': 1,\n",
       "           'it': 2,\n",
       "           'go': 1,\n",
       "           'someth': 1,\n",
       "           'like': 1,\n",
       "           'proc': 1,\n",
       "           'global': 1,\n",
       "           'msg': 2,\n",
       "           'statu': 2,\n",
       "           'clear': 1,\n",
       "           'unseen': 1,\n",
       "           'red': 1,\n",
       "           'mh': 1,\n",
       "           'setcur': 1,\n",
       "           'id': 3,\n",
       "           'set': 1,\n",
       "           'get': 2,\n",
       "           'busi': 1,\n",
       "           'pickmarkseen': 1,\n",
       "           'ok': 1,\n",
       "           'blue': 1,\n",
       "           'base': 1,\n",
       "           'upon': 1,\n",
       "           'gut': 1,\n",
       "           'markseen': 1,\n",
       "           'you': 1,\n",
       "           'fill': 1,\n",
       "           'section': 1,\n",
       "           'ran': 1,\n",
       "           'time': 1,\n",
       "           'john': 1,\n",
       "           'tm': 1,\n",
       "           'worker': 2,\n",
       "           'redhat': 1,\n",
       "           'com': 1,\n",
       "           'url': 1})]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_spam_token_counters[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8fc6287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REF: https://wkirgsn.github.io/2018/02/15/pandas-pipelines/    \n",
    "class BOWTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Transforms each email of a spam or ham directory into a Counter of counted tokens \"\"\"\n",
    "    def __init__(self, vocab=None, vocab_size=1000, min_count=7):\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = vocab_size\n",
    "        self.min_count = min_count\n",
    "        return None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def create_vocab(self, counters):\n",
    "        \n",
    "        full_vocab = sorted(sum(counters, Counter()).items(), key=lambda item: item[1], reverse=True)\n",
    "        full_vocab_notail = [w[0] for w in full_vocab if w[1] >= self.min_count]\n",
    "        vocab_size = min(self.vocab_size, len(full_vocab_notail))\n",
    "        return full_vocab_notail[:vocab_size]\n",
    "\n",
    "    def calculate_BOW(self, tokens: Counter):\n",
    "        bow_dict = dict.fromkeys(self.vocab,0)\n",
    "        for token, count in tokens.items():\n",
    "            if token in bow_dict:\n",
    "                bow_dict[token] = count\n",
    "        return bow_dict\n",
    "\n",
    "    \n",
    "    def transform(self, X):\n",
    "        bows = []\n",
    "        \n",
    "        if self.vocab == None:\n",
    "            self.vocab = self.create_vocab([x[1] for x in X])\n",
    "\n",
    "        \n",
    "        for x in X:\n",
    "            counter = x[1]\n",
    "            bow = self.calculate_BOW(counter)\n",
    "            bow['target_isSpam'] = x[0]\n",
    "            bows.append(bow)\n",
    "\n",
    "        df = pd.DataFrame(bows) \n",
    "        y, X = df.loc[:,'target_isSpam'], df.drop('target_isSpam', axis=1)\n",
    "        return y, X\n",
    "        \n",
    "\n",
    "         \n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5aff9ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_bow_transformer = BOWTransformer(vocab_size=500)\n",
    "y_train, X_train = training_bow_transformer.fit_transform(training_spam_token_counters)\n",
    "\n",
    "test_bow_transformer = BOWTransformer(vocab=list(X_train.columns))\n",
    "y_test, X_test = test_bow_transformer.fit_transform(test_spam_token_counters)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa89247",
   "metadata": {},
   "source": [
    "# Model the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e34116",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c254415",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] END ................................ score: (test=0.977) total time=   0.1s\n",
      "[CV] END ................................ score: (test=0.978) total time=   0.1s\n",
      "[CV] END ................................ score: (test=0.975) total time=   0.1s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9766584766584767"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "score = cross_val_score(log_clf, X_train, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cd44adb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 93.88%\n",
      "Recall: 92.00%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "log_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_analysis",
   "language": "python",
   "name": "python3_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6f33133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apache Spam Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799318e",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07d9e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "url = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
    "files = ['20021010_easy_ham.tar.bz2','20021010_spam.tar.bz2']\n",
    "    \n",
    "\n",
    "    \n",
    "def create_directory(cwd: str, path_extension: str):\n",
    "    new_path = cwd + path_extension\n",
    "    if not os.path.exists(new_path):\n",
    "        os.makedirs(new_path)\n",
    "\n",
    "\n",
    "        \n",
    "def dl_from_URL(urls: list, target_path: str):\n",
    "    print(f\"Start: {len(urls)} files to download\")\n",
    "    \n",
    "    for idx, url in tqdm(enumerate(urls)):\n",
    "        filename = url.split('/')[-1]\n",
    "        destination_path = target_path + filename\n",
    "        r = requests.get(url)\n",
    "        with open(destination_path, 'wb') as output_file:\n",
    "            output_file.write(r.content)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f5bccfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start: 2 files to download\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:03,  1.90s/it]\n"
     ]
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "source_path = '/data/source/'\n",
    "DATA_PATH = source_path \n",
    "urls = [url + file for file in files]\n",
    "\n",
    "\n",
    "\n",
    "create_directory(cwd, source_path)\n",
    "dl_from_URL(urls, cwd+DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cbd5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "def decompress_data(source_path: str, decmprsd_path: str):\n",
    "    \n",
    "    directory = os.fsencode(source_path)\n",
    "\n",
    "    for file in os.listdir(directory):\n",
    "        filename = os.fsdecode(file)\n",
    "        if filename.endswith(\".bz2\"): \n",
    "            tar = tarfile.open(source_path+filename, mode='r:bz2')\n",
    "            tar.extractall(decmprsd_path)\n",
    "        else:\n",
    "             continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a85aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "decmprsd_path = '/data/decompressed/'\n",
    "create_directory(cwd, decmprsd_path)\n",
    "decompress_data(cwd+DATA_PATH, cwd+decmprsd_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0620c3",
   "metadata": {},
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15a52c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import shutil\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_train_test_by_id(spam_path: str, ham_path: str, seed: int, training_path: str, test_path: str ):\n",
    "    random.seed(seed)\n",
    "    num_spam = len(os.listdir(spam_path))\n",
    "    num_ham = len(os.listdir(ham_path))\n",
    "    spam_test_indicies = random.sample([i for i in range(num_spam)], k=int(num_spam*.2))\n",
    "    ham_test_indicies = random.sample([i for i in range(num_ham)], k=int(num_ham*.2))\n",
    "\n",
    "\n",
    "    paths = [test_path, training_path]\n",
    "    for path in paths:\n",
    "        shutil.rmtree(path)\n",
    "        create_directory('', path)\n",
    "    \n",
    "    for file in os.listdir(spam_path):\n",
    "        filename = os.fsdecode(file)\n",
    "        _id = int(filename.split('.')[0])\n",
    "        if _id in spam_test_indicies:\n",
    "            os.popen(f\"cp {spam_path+filename} {test_path+filename+'.spam'}\") \n",
    "\n",
    "        else:\n",
    "            os.popen(f\"cp {spam_path+filename} {training_path+filename+'.spam'}\") \n",
    "\n",
    "        \n",
    "    for file in os.listdir(ham_path):\n",
    "        filename = os.fsdecode(file)\n",
    "        _id = int(filename.split('.')[0])\n",
    "        if _id in ham_test_indicies:\n",
    "            os.popen(f\"cp {ham_path+filename} {test_path+filename+'.ham'}\") \n",
    "\n",
    "        else:\n",
    "            os.popen(f\"cp {ham_path+filename} {training_path+filename+'.ham'}\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a7bcc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_path = cwd+decmprsd_path + 'spam/'\n",
    "ham_path = cwd+decmprsd_path + 'easy_ham/'\n",
    "\n",
    "\n",
    "\n",
    "training_path = '/data/training/source/'\n",
    "test_path = '/data/test/source/' \n",
    "create_directory(cwd, training_path)\n",
    "create_directory(cwd, test_path)\n",
    "\n",
    "\n",
    "split_train_test_by_id(spam_path, ham_path, 50, cwd+training_path, cwd+test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4d3f0ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/daltonsi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/daltonsi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", ' ', text.lower()) #alphanumeric lowercase\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [word for word in tokens if word not in stopwords.words(\"english\")]\n",
    "        tokens = [PorterStemmer().stem(w) for w in tokens]\n",
    "        return tokens\n",
    "\n",
    "def create_vocab(directory):\n",
    "    texts = []\n",
    "    vocab = []\n",
    "    \n",
    "    for file in os.listdir(directory):\n",
    "        \n",
    "        target = file.split('.')[-1]\n",
    "        try:\n",
    "            prcsd_text = process_text(open(directory+file).read())\n",
    "            texts.append((target, prcsd_text))\n",
    "            for token in prcsd_text:\n",
    "                if token not in vocab:\n",
    "                    vocab.append(token)\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    \n",
    "    \n",
    "    vocab.sort()\n",
    "    \n",
    "    return texts, vocab\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8374afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, vocab = create_vocab(cwd+training_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7ae2f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_BOW(vocab,doc):\n",
    "    tf_diz = dict.fromkeys(vocab,0)\n",
    "    for word in doc:\n",
    "        tf_diz[word]=doc.count(word)\n",
    "    return tf_diz\n",
    "\n",
    "def create_training_data(texts, vocab):\n",
    "    bows = []\n",
    "    for text in texts[:10]:\n",
    "        target = text[0]\n",
    "        doc = text[1]\n",
    "        bow = calculate_BOW(vocab, doc)\n",
    "        if target == 'spam':\n",
    "            bow['target_isSpam'] = 1\n",
    "        else:\n",
    "            bow['target_isSpam'] = 0\n",
    "        bows.append(bow)\n",
    "    \n",
    "    df = pd.DataFrame(bows) \n",
    "    \n",
    "    return df.loc[:,'target_isSpam'], df.drop('target_isSpam', axis=1)\n",
    "\n",
    " \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d37a9457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10,) (10, 56219)\n"
     ]
    }
   ],
   "source": [
    "y, X = create_training_data(texts,vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3308ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_analysis",
   "language": "python",
   "name": "python3_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

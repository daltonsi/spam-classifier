{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f33133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apache Spam Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b799318e",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "07d9e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "URL_ROOT = 'https://spamassassin.apache.org/old/publiccorpus/'\n",
    "HAM_FILE = '20021010_easy_ham.tar.bz2'\n",
    "SPAM_FILE = '20021010_spam.tar.bz2'\n",
    "\n",
    "\n",
    "MAIN_PATH = os.getcwd()\n",
    "SOURCE_PATH = os.path.join(MAIN_PATH, 'data/source/')\n",
    "\n",
    "def reset_directory(directory):\n",
    "    if os.path.exists(directory):\n",
    "        shutil.rmtree(directory)\n",
    "    os.makedirs(directory) \n",
    "\n",
    "def download_spam():\n",
    "    \"\"\" Download Spam/Ham sources into local directory \"\"\"\n",
    "    reset_directory(SOURCE_PATH)\n",
    "    \n",
    "    for filename in [HAM_FILE, SPAM_FILE]:\n",
    "        r = requests.get(URL_ROOT + filename)\n",
    "        filepath = os.path.join(SOURCE_PATH, filename)\n",
    "        with open(filepath, 'wb') as destination:\n",
    "            destination.write(r.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6f5bccfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_spam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4cbd5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "EMAILS_PATH = os.path.join(SOURCE_PATH, 'emails/')\n",
    "\n",
    "def extract_spam():\n",
    "    \"\"\" Extracts spam email files from compressed directory \"\"\"\n",
    "    reset_directory(EMAILS_PATH)\n",
    "    \n",
    "    for file in os.listdir(SOURCE_PATH):\n",
    "        if file.endswith(\".bz2\"): \n",
    "            filepath = os.path.join(SOURCE_PATH, file)\n",
    "            tar = tarfile.open(filepath, mode='r:bz2')\n",
    "            tar.extractall(EMAILS_PATH)\n",
    "        else:\n",
    "             continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0a85aa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_spam()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0620c3",
   "metadata": {},
   "source": [
    "# Prepare the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "15a52c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import glob\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "TRAINING_PATH = os.path.join(MAIN_PATH, 'data/training/source/')\n",
    "TEST_PATH = os.path.join(MAIN_PATH, 'data/test/source/')\n",
    "\n",
    "EMAIL_SPAM_PATH = os.path.join(EMAILS_PATH, 'spam/')\n",
    "EMAIL_HAM_PATH = os.path.join(EMAILS_PATH, 'easy_ham/')\n",
    "\n",
    "NUM_SPAM = len(os.listdir(EMAIL_SPAM_PATH))\n",
    "NUM_HAM = len(os.listdir(EMAIL_HAM_PATH))\n",
    "\n",
    "\n",
    "\n",
    "def split_spam_train_test(seed=50, percent_test=20):\n",
    "    \"\"\" Splits spam email files into test/train directories \"\"\"\n",
    "    reset_directory(TRAINING_PATH)\n",
    "    reset_directory(TEST_PATH)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    spam_test_indcs = random.sample([i for i in range(NUM_SPAM)], k=int(NUM_SPAM*percent_test/100))\n",
    "    ham_test_indcs = random.sample([i for i in range(NUM_HAM)], k=int(NUM_HAM*percent_test/100))\n",
    "    \n",
    "    for file in os.listdir(EMAIL_SPAM_PATH):\n",
    "        _id = int(file.split('.')[0])\n",
    "        source_path = os.path.join(EMAIL_SPAM_PATH, file)\n",
    "\n",
    "        if _id in spam_test_indcs:\n",
    "            destination = os.path.join(TEST_PATH, file + '.spam')\n",
    "            os.popen(f\"cp {source_path} {destination}\") \n",
    "\n",
    "        else:\n",
    "            destination = os.path.join(TRAINING_PATH, file + '.spam')\n",
    "            os.popen(f\"cp {source_path} {destination}\") \n",
    "\n",
    "        \n",
    "    for file in os.listdir(EMAIL_HAM_PATH):\n",
    "        _id = int(file.split('.')[0])\n",
    "        source_path = os.path.join(EMAIL_HAM_PATH, file)\n",
    "\n",
    "        if _id in ham_test_indcs:\n",
    "            destination = os.path.join(TEST_PATH, file + '.ham')\n",
    "            os.popen(f\"cp {source_path} {destination}\") \n",
    "\n",
    "        else:\n",
    "            destination = os.path.join(TRAINING_PATH, file + '.ham')\n",
    "            os.popen(f\"cp {source_path} {destination}\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a7bcc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_spam_train_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "1468c995",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import email\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class EmailParser():\n",
    "    def __init__(self, file_path):\n",
    "        self.y = isSpam(file_path.split('.')[-1])\n",
    "        self.mail_objc = self.open_mail_objc(file_path)\n",
    "        self.X = None\n",
    "        \n",
    "    \n",
    "    def isSpam(self, spam_string):\n",
    "        if spam_string == 'spam':\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def open_mail_objc(self, file_path):\n",
    "        msg_str = open(file_path, encoding='latin1').read()\n",
    "        return email.message_from_string(msg_str)\n",
    "        \n",
    "    def process_mail_component(self, mail_objc):\n",
    "        \n",
    "        part_type = mail_objc.get_content_type()\n",
    "        if part_type in ['text/plain']:\n",
    "            return mail_objc.get_payload()\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "    def build_msg_content(self):\n",
    "        print('hello')\n",
    "        prcssd_msg = ''\n",
    "        \n",
    "        mail_objc = self.mail_objc\n",
    "        if mail_objc.is_multipart():\n",
    "            for msg in mail_objc.walk():\n",
    "                payload = self.process_mail_component(msg)\n",
    "                if payload:\n",
    "                    prcssd_msg += payload\n",
    "    \n",
    "\n",
    "            return prcssd_msg\n",
    "        \n",
    "        else:\n",
    "\n",
    "            return self.target, self.process_mail_component(mail_objc)\n",
    "            \n",
    "    \n",
    "             \n",
    "\n",
    "\n",
    "\n",
    "def spam_email_transform(directory):\n",
    "    print('hi')\n",
    "    target_tokenset_pairs = []\n",
    "    \n",
    "    for file in os.listdir(directory)[0:2]:\n",
    "        if not file.startswith('.'):\n",
    "            file_path = os.path.join(directory + file)\n",
    "            email_parser = EmailParser(file_path)\n",
    "            y, final_msg_str = email_parser.build_msg_content()\n",
    "            print(final_msg_str)\n",
    "            print()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "44ecc8eb",
   "metadata": {},
   "source": [
    "spam_email_transform(TRAINING_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f0ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import email\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "def tokenize_text(text: str, remove_stop=True, agg_ints=True, agg_leading_zeros=True, stemmer=True):\n",
    "        \n",
    "        text, count = re.subn(r\"[^a-zA-Z0-9]\", ' ', text) #alphanumeric lowercase\n",
    "\n",
    "\n",
    "        tokens = word_tokenize(text)\n",
    "        if agg_ints:\n",
    "            tokens = [re.sub(r\"^[0-9]+$\", 'INTEGER', token) for token in tokens]\n",
    "        if agg_leading_zeros:\n",
    "            tokens = [re.sub(r\"^[0]+.*\", 'LEADING_ZERO', token) for token in tokens]\n",
    "        if remove_stop:\n",
    "            tokens = [token for token in tokens if token not in stopwords.words(\"english\")]\n",
    "        if stemmer:\n",
    "            tokens = [PorterStemmer().stem(w) for w in tokens]\n",
    "        return tokens\n",
    "\n",
    "    \n",
    "def parse_msg_list(msg_list: list):\n",
    "    full_msg = ''\n",
    "    for msg in msg_list:\n",
    "        msg = msg.get_payload()\n",
    "        if type(msg) == list:\n",
    "            full_msg += parse_msg_list(msg)\n",
    "        else:\n",
    "            full_msg += msg\n",
    "    return full_msg\n",
    "\n",
    "\n",
    "    \n",
    "def create_target_tokenset_pairs(directory: str):\n",
    "    target_tokenset_pairs = []\n",
    "    \n",
    "    for file_path in os.listdir(directory):\n",
    "        if not file_path.startswith('.'):\n",
    "            target = file_path.split('.')[-1]\n",
    "            msg_str = open(directory+file_path, encoding='latin1').read()\n",
    "            payload = email.message_from_string(msg_str).get_payload()\n",
    "            if payload == list:\n",
    "                full_msg = parse_msg_list(payload)\n",
    "            else:\n",
    "                full_msg = payload\n",
    "            try:\n",
    "                msg_tokens = tokenize_text(full_msg)\n",
    "            except:\n",
    "                print(type(full_msg))\n",
    "                print(full_msg)\n",
    "                print()\n",
    "            target_tokenset_pairs.append((target, msg_tokens))\n",
    "    return target_tokenset_pairs\n",
    "\n",
    "\n",
    "\n",
    "def create_wordset(list_of_target_tokensets:list):\n",
    "    return sorted(list(set([token for tokenset in list_of_target_tokensets for token in tokenset])))\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_target_tokenset_pairs = create_target_tokenset_pairs(cwd+training_path)\n",
    "test_target_tokenset_pairs = create_target_tokenset_pairs(cwd+test_path)\n",
    "\n",
    "\n",
    "training_wordset = create_wordset([t[1] for t in training_target_tokenset_pairs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d41be74",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_wordset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae2f719",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_BOW(vocab:list, tokens: list):\n",
    "    bow_dict = dict.fromkeys(vocab,0)\n",
    "    for token in tokens:\n",
    "        bow_dict[token]=tokens.count(token)\n",
    "    return bow_dict\n",
    "\n",
    "def create_bow_df(target_token_pairs:list, vocab:list):\n",
    "    bows = []\n",
    "    for pair in target_token_pairs[:10]:\n",
    "        target = pair[0]\n",
    "        tokens = pair[1]\n",
    "        bow = calculate_BOW(vocab, tokens)\n",
    "        if target == 'spam':\n",
    "            bow['target_isSpam'] = 1\n",
    "        else:\n",
    "            bow['target_isSpam'] = 0\n",
    "        bows.append(bow)\n",
    "    \n",
    "    df = pd.DataFrame(bows) \n",
    "    \n",
    "    return df.loc[:,'target_isSpam'], df.drop('target_isSpam', axis=1)\n",
    "\n",
    " \n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a30a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train, X_train = create_bow_df(training_target_tokenset_pairs, vocab)\n",
    "y_test, X_test = create_bow_df(test_target_tokenset_pairs, vocab)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b879077e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = '/data/training/processed/'\n",
    "test_path = '/data/test/processed/' \n",
    "create_directory(cwd, training_path)\n",
    "create_directory(cwd, test_path)\n",
    "\n",
    "\n",
    "y_train.to_csv(cwd+training_path+'y_train.csv')\n",
    "X_train.to_csv(cwd+training_path+'X_train.csv')\n",
    "\n",
    "X_test.to_csv(cwd+test_path+'X_test.csv')\n",
    "y_test.to_csv(cwd+test_path+'y_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd283678",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f6e5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65bc7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81a8d87",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3_analysis",
   "language": "python",
   "name": "python3_analysis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
